{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "Min-Max scaling is a data preprocessing technique used to transform the features of a dataset into a specific range, typically between 0 and 1. It's done by scaling each feature so that its minimum value becomes 0, and its maximum value becomes 1, while values in between are linearly scaled accordingly. This technique is useful when you want to ensure that all features contribute equally to the analysis, and it can help in cases where some features have different scales.\n",
    "\n",
    "Example:\n",
    "Let's say you have a dataset of exam scores with scores ranging from 40 to 95. Applying Min-Max scaling would transform the data so that 40 becomes 0, 95 becomes 1, and all other scores are scaled accordingly within this range. So, if a score was originally 75, it would be scaled to (75-40)/(95-40) = 0.5 after Min-Max scaling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "The Unit Vector technique in feature scaling, also known as normalization, scales the features of a dataset to have a unit norm, which means that each feature vector is transformed into a vector with a length of 1. This technique differs from Min-Max scaling because it doesn't force the data into a specific range but rather ensures that all feature vectors have the same length.\n",
    "\n",
    "Example:\n",
    "Suppose you have a dataset with two features, A and B, and you want to normalize the feature vectors. If the original feature vector [A, B] is [3, 4], the normalized feature vector would be [0.6, 0.8]. Here, the length of the vector [0.6, 0.8] is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "PCA, or Principal Component Analysis, is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional representation while preserving as much variance as possible. It does this by identifying and extracting the principal components, which are linear combinations of the original features. PCA is often used to reduce the dimensionality of data for analysis and visualization.\n",
    "\n",
    "Example:\n",
    "Suppose you have a dataset with 20 features representing different aspects of a person's health. Using PCA, you can reduce the dimensionality to, let's say, 5 principal components. These components are linear combinations of the original features, and they capture the most significant variation in the data. You can then use these 5 components for further analysis or visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "PCA is a form of feature extraction. It extracts linear combinations of the original features, called principal components, which are ordered by the amount of variance they explain. By choosing a subset of these principal components, you can effectively perform feature extraction. For example, if you only select the top k principal components out of n, where k < n, you have effectively reduced the dimensionality of your dataset while retaining the most important information.\n",
    "\n",
    "Example:\n",
    "Suppose you have a dataset with 100 features representing images of handwritten digits. By applying PCA, you extract the top 10 principal components, which capture the most important patterns in the images. You can then use these 10 components as features for a classification task, effectively reducing the dimensionality from 100 to 10 while preserving the essential information needed for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "To preprocess data for a recommendation system using Min-Max scaling, you would follow these steps:\n",
    "\n",
    "1.Identify the features you want to scale, such as price, rating, and delivery time.\n",
    "\n",
    "2.Determine the minimum and maximum values for each feature in your dataset.\n",
    "\n",
    "3.For each feature, apply the Min-Max scaling formula:\n",
    "\n",
    "4.Scaled Value = (Original Value - Min) / (Max - Min)\n",
    "\n",
    "5.Repeat step 3 for each data point in your dataset.\n",
    "\n",
    "Your scaled data will now have values in the range [0, 1]. This ensures that all features contribute equally to the recommendation system, regardless of their original scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6\n",
    "To reduce the dimensionality of a dataset containing multiple features for predicting stock prices using PCA, follow these steps:\n",
    "\n",
    "1.Standardize the data: Ensure that all features have zero mean and unit variance. This step is crucial because PCA is sensitive to the scale of the features.\n",
    "\n",
    "2.Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "3.Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "4.Sort the eigenvectors by their corresponding eigenvalues in descending order. The eigenvectors with the largest eigenvalues represent the principal components.\n",
    "\n",
    "5.Choose the desired number of principal components to retain. This choice can be based on the explained variance, where you aim to retain a certain percentage of the total variance.\n",
    "\n",
    "6.Project your original data onto the selected principal components to obtain a lower-dimensional representation.\n",
    "\n",
    "7.You can use this reduced-dimensional data for building your stock price prediction model.\n",
    "\n",
    "The number of principal components to retain depends on your specific goals and the amount of variance you are willing to preserve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7\n",
    "To perform Min-Max scaling on the dataset [1, 5, 10, 15, 20] to transform the values to a range of -1 to 1, you can follow these steps:\n",
    "\n",
    "Determine the minimum and maximum values in the dataset:\n",
    "\n",
    "Min = 1\n",
    "Max = 20\n",
    "Apply the Min-Max scaling formula for each value in the dataset:\n",
    "\n",
    "Scaled Value = (Original Value - Min) / (Max - Min)\n",
    "\n",
    "For 1: (-1)\n",
    "\n",
    "For 5: (-0.5)\n",
    "\n",
    "For 10: (0)\n",
    "\n",
    "For 15: (0.5)\n",
    "\n",
    "For 20: (1)\n",
    "\n",
    "So, the Min-Max scaled values will be [-1, -0.5, 0, 0.5, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8\n",
    "To perform feature extraction using PCA on the dataset [height, weight, age, gender, blood pressure], you would follow these steps:\n",
    "\n",
    "1.Standardize the data: Ensure that all features have zero mean and unit variance.\n",
    "\n",
    "2.Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "3.Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "4.Sort the eigenvectors by their corresponding eigenvalues in descending order. These eigenvectors represent the principal components.\n",
    "\n",
    "5.Decide how many principal components to retain based on the explained variance or other criteria. The choice may involve retaining a sufficient percentage of the total variance, e.g., 95% or 99%.\n",
    "\n",
    "6.Select the top principal components according to your chosen criterion.\n",
    "\n",
    "The number of principal components to retain will depend on the level of dimensionality reduction you want to achieve while still preserving enough information for your specific analysis or modeling needs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
